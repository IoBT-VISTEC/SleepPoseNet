{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxdn8iHMG8n2"
      },
      "source": [
        "## SleepPoseNet\r\n",
        "The code below is the example of running *Experiment IV* in the paper that using *Dataset II*. Other different settings in the paper can be done by modifying this code. If you have any enquiry, \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upo8sUBk5fUy"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from matplotlib.animation import FuncAnimation\r\n",
        "from IPython.display import HTML\r\n",
        "from scipy.fftpack import fft, fftshift\r\n",
        "from scipy.signal import butter, lfilter\r\n",
        "import matplotlib.lines as mlines\r\n",
        "import math\r\n",
        "from numpy import dstack\r\n",
        "import numpy.matlib\r\n",
        "from pandas import read_csv\r\n",
        "from skimage import img_as_ubyte\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.metrics import classification_report, confusion_matrix\r\n",
        "from sklearn.metrics import precision_recall_fscore_support\r\n",
        "from skimage import filters\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.externals import joblib\r\n",
        "from sklearn.model_selection import PredefinedSplit\r\n",
        "\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras import models\r\n",
        "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, UpSampling2D, Flatten, Input, Conv2D, Conv2DTranspose, LeakyReLU, PReLU, Lambda, add\r\n",
        "from tensorflow.keras.models import Model, load_model\r\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\r\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "import scipy\r\n",
        "from scipy.fftpack import fft\r\n",
        "from scipy.fftpack import fft2\r\n",
        "from scipy import signal\r\n",
        "from scipy.interpolate import CubicSpline\r\n",
        "from collections import Counter\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "#Essential parameters\r\n",
        "fBin = 0 #First range bin\r\n",
        "lBin = 180 #Last range bin\r\n",
        "nBin = 40 #The total number of range bins\r\n",
        "time_window = 100 #The total number of slow time indices\r\n",
        "\r\n",
        "#WRTFT PARAMETERS\r\n",
        "NFFT = 25\r\n",
        "stride  = 2\r\n",
        "outdim_size = 20\r\n",
        "\r\n",
        "#Hyperparameter of CNN\r\n",
        "PATIENCE = 200 #Early stopping\r\n",
        "EPOCHS = 1000 \r\n",
        "BATCH_SIZE = 16\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGbpFiDxEJ0r"
      },
      "source": [
        "# CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcW8hln2D_Vl"
      },
      "source": [
        "\r\n",
        "#SleepPoseNet Model\r\n",
        "def make_model():\r\n",
        "\r\n",
        "  #TD input\r\n",
        "  amp_input = layers.Input(shape=(nBin, 99,1), name = 'amp_input')\r\n",
        "  x_amp = layers.Conv2D(16, (2, 3), activation='relu', padding='valid', strides = (1,1), kernel_regularizer=tf.keras.regularizers.l2(0.0005))(amp_input)\r\n",
        "  x_amp = layers.SpatialDropout2D(0.3)(x_amp)\r\n",
        "  x_amp = layers.MaxPooling2D((2, 3))(x_amp)\r\n",
        "  x_amp = layers.Conv2D(32, (2, 3),  activation='relu', padding='valid', strides = (1,1), kernel_regularizer=tf.keras.regularizers.l2(0.0005))(x_amp)\r\n",
        "  x_amp = layers.SpatialDropout2D(0.3)(x_amp)\r\n",
        "  x_amp = layers.MaxPooling2D((2, 3))(x_amp)\r\n",
        "  x_amp = layers.Conv2D(32, (2, 3),  activation='relu', padding='valid', strides = (1,1), kernel_regularizer=tf.keras.regularizers.l2(0.0005))(x_amp)\r\n",
        "  x_amp = layers.SpatialDropout2D(0.3)(x_amp)\r\n",
        "  x_amp = layers.MaxPooling2D((2, 3))(x_amp)\r\n",
        "  x_amp = layers.Flatten()(x_amp)\r\n",
        "  amp_flat_shape = x_amp.shape\r\n",
        "  x_amp = layers.Dense(outdim_size, kernel_regularizer=tf.keras.regularizers.l2(0.01))(x_amp)\r\n",
        "\r\n",
        "  #WRTFT Input\r\n",
        "  wrtft_input = layers.Input(shape=(NFFT//2+1, (time_window-NFFT)//stride+1,1))\r\n",
        "  x_wrtft = layers.Conv2D(10, (2,2), activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.0005))(wrtft_input)\r\n",
        "  x_wrtft = layers.SpatialDropout2D(0.3)(x_wrtft)\r\n",
        "  x_wrtft = layers.MaxPooling2D((2, 2))(x_wrtft)\r\n",
        "  x_wrtft = layers.Conv2D(20, (2, 2), activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.0005))(x_wrtft)\r\n",
        "  x_wrtft = layers.SpatialDropout2D(0.3)(x_wrtft)\r\n",
        "  x_wrtft = layers.MaxPooling2D((2, 2))(x_wrtft)\r\n",
        "  x_wrtft = layers.Flatten()(x_wrtft)\r\n",
        "  wrtft_flat_shape = x_wrtft.shape\r\n",
        "  x_wrtft = layers.Dense(outdim_size, kernel_regularizer=tf.keras.regularizers.l2(0.01))(x_wrtft)\r\n",
        "\r\n",
        "\r\n",
        "  #Fuse two features\r\n",
        "  concat_output = layers.concatenate([x_amp, x_wrtft], name='cca_output')\r\n",
        "\r\n",
        "  #ouput postures = 5 classes\r\n",
        "  output_posture = layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2(0.1))(concat_output)\r\n",
        "  output_posture = layers.ReLU()(output_posture)\r\n",
        "  \r\n",
        "  output_posture = layers.Dropout(0.5)(output_posture)\r\n",
        "  output_posture = layers.Dense(5, activation='softmax', name='posture_output')(output_posture)\r\n",
        "\r\n",
        "  dcnn_model = Model(inputs=[amp_input, wrtft_input], outputs=output_posture)\r\n",
        "\r\n",
        "  return dcnn_model\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYUpdFo_ERaW"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEZiKbk5D_34"
      },
      "source": [
        "#Preprocessing functions\r\n",
        "#Remove DC noise\r\n",
        "def time_avg(x):\r\n",
        "  x_temp = np.copy(x)\r\n",
        "  for i in range(x.shape[0]):\r\n",
        "    for s in range(x.shape[1]):\r\n",
        "      x_avg = x_temp[i,s,:].mean()\r\n",
        "      x[i, s, :] = x[i,s,:] - x_avg\r\n",
        "  return x\r\n",
        "\r\n",
        "#Remove scatter effect from other objects\r\n",
        "def range_avg(x):\r\n",
        "  x_temp = np.copy(x)\r\n",
        "  for i in range(x.shape[0]):\r\n",
        "    for s in range(x.shape[2]):\r\n",
        "      x_avg = x_temp[i,:,s].mean()\r\n",
        "      x[i, :, s] = x[i,:,s] - x_avg\r\n",
        "  return x\r\n",
        "\r\n",
        "#Range Selection\r\n",
        "def crop_by_highest_sum(X, X_diff, spatial_size):\r\n",
        "  spatial_highest_position = list()\r\n",
        "  spatial_end = X_diff.shape[1]-spatial_size+1\r\n",
        "  spatial_images_crop = list()\r\n",
        "\r\n",
        "  #Spatial crop\r\n",
        "  for i in range (X.shape[0]):\r\n",
        "    spatial_max_value = -1000\r\n",
        "    for j in range (10, spatial_end):\r\n",
        "      current_sum = np.sum(X_diff[i, j:j+spatial_size, :]**2)\r\n",
        "      if current_sum>=spatial_max_value:\r\n",
        "        spatial_position = j\r\n",
        "        spatial_max_value = current_sum\r\n",
        "    spatial_images_crop.append(X[i, spatial_position:spatial_position+spatial_size, :])\r\n",
        "    spatial_highest_position.append(spatial_position)\r\n",
        "\r\n",
        "  X_crop = np.array(spatial_images_crop)\r\n",
        "  return X_crop, spatial_highest_position"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SateTT5YEbbW"
      },
      "source": [
        "## Data Augmentation\r\n",
        "\r\n",
        "Our code is modified from the [Original Code](https://github.com/terryum/Data-Augmentation-For-Wearable-Sensor-Data) which is used for wearable sensors\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WkvNx8vD6x5"
      },
      "source": [
        "#Data Augmentation functions\r\n",
        "def signal_shift(x,shft_arr):\r\n",
        "    res = []\r\n",
        "    for s in shft_arr:\r\n",
        "        zero = np.zeros((x.shape[0],abs(s)))\r\n",
        "        tmp = x\r\n",
        "        if s > 0:\r\n",
        "            tmp = np.concatenate((zero,x[:,:-s]),axis = 1)\r\n",
        "        elif s < 0:\r\n",
        "            tmp = np.concatenate((x[:,-s:],zero),axis = 1)\r\n",
        "        res.append(tmp)\r\n",
        "    return res\r\n",
        "\r\n",
        "def signal_shift_time(x,shft_arr):\r\n",
        "    res = []\r\n",
        "    x = x.T\r\n",
        "    for s in shft_arr:\r\n",
        "        zero = np.zeros((x.shape[0],abs(s)))\r\n",
        "        tmp = x\r\n",
        "        if s > 0:\r\n",
        "            tmp = np.concatenate((zero,x[:,:-s]),axis = 1)\r\n",
        "        elif s < 0:\r\n",
        "            tmp = np.concatenate((x[:,-s:],zero),axis = 1)\r\n",
        "        tmp = tmp.T\r\n",
        "        res.append(tmp)\r\n",
        "    return res\r\n",
        "\r\n",
        "def GenerateRandomCurves(X, sigma, knot=4):\r\n",
        "    xx = (np.ones((X.shape[1],1))*(np.arange(0,X.shape[0], (X.shape[0]-1)/(knot+1)))).transpose()\r\n",
        "    yy = np.random.normal(loc=1.0, scale=sigma, size=(knot+2, 1))\r\n",
        "    x_range = np.arange(X.shape[0])\r\n",
        "    cs_x = CubicSpline(xx[:,0], yy[:,0])\r\n",
        "    return np.array(cs_x(x_range)).transpose()\r\n",
        "\r\n",
        "def DistortTimesteps(X, sigma):\r\n",
        "    tt = GenerateRandomCurves(X, sigma) # Regard these samples aroun 1 as time intervals\r\n",
        "    tt_cum = np.cumsum(tt, axis=0)        # Add intervals to make a cumulative graph\r\n",
        "    # Make the last value to have X.shape[0]\r\n",
        "    t_scale = (X.shape[0]-1)/tt_cum[-1]\r\n",
        "    tt_cum = tt_cum*t_scale\r\n",
        "    return tt_cum\r\n",
        "\r\n",
        "def DA_TimeWarp(X, sigma):\r\n",
        "    tt_new = DistortTimesteps(X, sigma)\r\n",
        "    X_new = np.zeros(X.shape)\r\n",
        "    x_range = np.arange(X.shape[0])\r\n",
        "    for i in range(X.shape[1]):\r\n",
        "        X_new[:,i] = np.interp(x_range, tt_new, X[:,i])\r\n",
        "    return X_new\r\n",
        "\r\n",
        "def DA_MagWarp(X, sigma):\r\n",
        "  X_new = np.zeros(X.shape)\r\n",
        "  for i in range (X.shape[1]):\r\n",
        "    X_new[:, i] = X[:, i] * GenerateRandomCurves(X, sigma)\r\n",
        "  return X_new\r\n",
        "    \r\n",
        "\r\n",
        "def increase_by_timewarp(dat_x, dat_y):  \r\n",
        "  dat_x_aug = []\r\n",
        "  dat_y_aug = []\r\n",
        "  timewarp_array = [0, 0.4]\r\n",
        "\r\n",
        "  for smp in dat_x:\r\n",
        "    for rep in timewarp_array:\r\n",
        "      if rep  != 0:\r\n",
        "        dat_x_aug += [DA_TimeWarp(smp.T,rep)]\r\n",
        "      else:\r\n",
        "        dat_x_aug += [smp.T]\r\n",
        "\r\n",
        "  for y in dat_y:\r\n",
        "    dat_y_aug += [y]*len(timewarp_array)\r\n",
        "\r\n",
        "  return np.swapaxes(np.array(dat_x_aug), 1 ,2), np.array(dat_y_aug)\r\n",
        "\r\n",
        "def increase_by_magwarp(dat_x, dat_y):  \r\n",
        "  dat_x_aug = []\r\n",
        "  dat_y_aug = []\r\n",
        "  magwarp_array = [0, 0.4]\r\n",
        "\r\n",
        "  for smp in dat_x:\r\n",
        "    for rep in magwarp_array:\r\n",
        "      if rep  != 0:\r\n",
        "        dat_x_aug += [DA_MagWarp(smp.T, rep)]\r\n",
        "      else:\r\n",
        "        dat_x_aug += [smp.T]\r\n",
        "\r\n",
        "  for y in dat_y:\r\n",
        "    dat_y_aug += [y]*len(magwarp_array)\r\n",
        "\r\n",
        "  return np.swapaxes(np.array(dat_x_aug), 1 ,2), np.array(dat_y_aug)\r\n",
        "  \r\n",
        "def increase_by_shift_time(dat_x, dat_y):\r\n",
        "  dat_x_aug = []\r\n",
        "  dat_y_aug = []\r\n",
        "  shft_array_time = [0,-5, 5, -10, 10]\r\n",
        "  for smp in dat_x:\r\n",
        "    dat_x_aug += signal_shift_time(smp.T,shft_array_time)\r\n",
        "\r\n",
        "  for y in dat_y:\r\n",
        "    dat_y_aug += [y]*len(shft_array_time)\r\n",
        "  \r\n",
        "  return np.swapaxes(np.array(dat_x_aug), 1 ,2), np.array(dat_y_aug)\r\n",
        "\r\n",
        "def increase_by_shift_range(dat_x, dat_y):\r\n",
        "  dat_x_aug = []\r\n",
        "  dat_y_aug = []\r\n",
        "  shft_array = [0,2,4]\r\n",
        "\r\n",
        "  for smp in dat_x:\r\n",
        "    dat_x_aug += signal_shift(smp.T,shft_array)\r\n",
        "\r\n",
        "  for y in dat_y:\r\n",
        "    dat_y_aug += [y]*len(shft_array)\r\n",
        "  return np.swapaxes(np.array(dat_x_aug), 1 ,2), np.array(dat_y_aug)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkbfB0kMFFJW"
      },
      "source": [
        "## Weighted Range-Time-Frequency Transform (WRTFT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZFF7DUP53Km"
      },
      "source": [
        "\r\n",
        "#WRTFT functions\r\n",
        "def plot_wrtft(F):\r\n",
        "    plt.figure(figsize=(10,4))\r\n",
        "    plt.imshow(F, cmap='hot', aspect='auto', interpolation='catrom')\r\n",
        "    plt.colorbar()\r\n",
        "    plt.gca().invert_yaxis()\r\n",
        "    plt.ylabel('Frequency')\r\n",
        "    plt.xlabel('Time')\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "def wrtft(X):\r\n",
        "    R = X # select frame range \r\n",
        "    val = filters.threshold_otsu(R)\r\n",
        "    B = np.copy(R)\r\n",
        "\r\n",
        "    for i in range(R.shape[0]):\r\n",
        "        for j in range(R.shape[1]):\r\n",
        "            if R[i][j] < val:\r\n",
        "                B[i][j] = 0\r\n",
        "            else :\r\n",
        "                B[i][j] = 1\r\n",
        "\r\n",
        "    min_r = 200\r\n",
        "    max_r = -1\r\n",
        "    for i in range(B.shape[0]):\r\n",
        "        for j in range(B.shape[1]):\r\n",
        "            if B[i][j]:\r\n",
        "                min_r = min(min_r,i)\r\n",
        "                max_r = max(max_r,i)\r\n",
        "    Em = []\r\n",
        "    for i in range(R.shape[0]):\r\n",
        "        Em.append(np.sum(R[i,:]**2))\r\n",
        "    Em = np.array(Em)            \r\n",
        "    sigma = Em[min_r:max_r]/sum(Em[min_r:max_r]) \r\n",
        "\r\n",
        "    F = np.zeros((NFFT//2+1,(R.shape[1]-NFFT)//stride+1))\r\n",
        "    for m in range(min_r,max_r):\r\n",
        "        _,_,arr = signal.stft(R[m,:], fs = 1, nperseg=NFFT, noverlap = NFFT - stride, padded = False, boundary = None)\r\n",
        "        arr = np.abs(arr)\r\n",
        "        F += sigma[m-min_r]*arr\r\n",
        "        del arr\r\n",
        "    return F\r\n",
        "from tqdm import tqdm\r\n",
        "def apply_wrtft(X):\r\n",
        "    res = []\r\n",
        "    cnt = 0\r\n",
        "    for dat in tqdm(X,position = 0):\r\n",
        "        F = wrtft(dat)\r\n",
        "        res.append(F)\r\n",
        "        cnt += 1\r\n",
        "    return np.array(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn_IN8SkGeSo"
      },
      "source": [
        "## Data loading and preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZuwgkR3GdtE"
      },
      "source": [
        "\r\n",
        "\r\n",
        "# Remove side-to-prone and prone-to-side classes (In the paper we use only 5 classes)\r\n",
        "def remove_class(y):\r\n",
        "  idx = list()\r\n",
        "  for i in range(y.shape[0]):\r\n",
        "    if not(y[i]==1 or y[i]==2):\r\n",
        "      idx.append(i)\r\n",
        "  return idx\r\n",
        "def transform_class(y):\r\n",
        "  # 0 = Supine side\r\n",
        "  # 1 = Supine prone\r\n",
        "  # 2 = Side supine\r\n",
        "  # 3 = Prone supine\r\n",
        "  # 4 = Background\r\n",
        "  y_out = list()\r\n",
        "  for i in range (len(y)):\r\n",
        "    if y[i]==0: #Supine side\r\n",
        "      y_out.append(0)\r\n",
        "    elif y[i]==3: #Side supine\r\n",
        "      y_out.append(2)\r\n",
        "    elif y[i]==4: #Supine prone\r\n",
        "      y_out.append(1)\r\n",
        "    elif y[i]==5: #Prone supine\r\n",
        "      y_out.append(3)\r\n",
        "    elif y[i]==6: #Background\r\n",
        "      y_out.append(4)\r\n",
        "\r\n",
        "  return np.array(y_out)\r\n",
        "\r\n",
        "#Load data\r\n",
        "prefix='/content/drive/My Drive/Colab Notebooks/Sleep Radar/'\r\n",
        "data_path=prefix+'2020 Data/radar_data/'\r\n",
        "model_path =prefix+'model/best_model.h5'\r\n",
        "dat_x_session1 = np.load(data_path+'X_wall_session1.npy')\r\n",
        "dat_x_session2 = np.load(data_path+'X_wall_session2.npy')\r\n",
        "y_session1 = np.load(data_path+'y_digit_session1.npy')\r\n",
        "y_session2 = np.load(data_path+'y_digit_session2.npy')\r\n",
        "sub_map_session1 = np.load(data_path+'subjects_session1.npy')\r\n",
        "sub_map_session2 = np.load(data_path+'subjects_session2.npy')\r\n",
        "dat_x = np.concatenate((dat_x_session1, dat_x_session2), axis=0)\r\n",
        "y = np.concatenate((y_session1, y_session2), axis=0)\r\n",
        "sub_map = np.concatenate((sub_map_session1, sub_map_session2)) \r\n",
        "session_label = np.array([1]*(y_session1.shape[0]) + [2]*(y_session2.shape[0]))\r\n",
        "rmv_idx = remove_class(y) #Remove side prone and prone side\r\n",
        "dat_x = dat_x[rmv_idx]\r\n",
        "y = transform_class(y[rmv_idx])\r\n",
        "sub_map = sub_map[rmv_idx]\r\n",
        "session_label = session_label[rmv_idx]\r\n",
        "\r\n",
        "# # preprocess X\r\n",
        "X = np.swapaxes(dat_x,1,2)\r\n",
        "X = X[:,:,30:130]\r\n",
        "X_amp = np.abs(X)\r\n",
        "X_amp = time_avg(X_amp)\r\n",
        "X_amp = range_avg(X_amp)\r\n",
        "X_amp_map = np.diff(X_amp, n=1, axis=2)\r\n",
        "X_amp, positions = crop_by_highest_sum(X_amp, X_amp_map, nBin)\r\n",
        "space, time = X_amp.shape[1], X_amp.shape[2]\r\n",
        "\r\n",
        "\r\n",
        "###############Prepare data for 10 folds####################\r\n",
        "#Prepare 10 fold datasets\r\n",
        "# get all data for multiple subject\r\n",
        "def data_for_subjects(sub_map, sub_id):\r\n",
        "    # get row indexes for the subject id\r\n",
        "    ix = [i for i in range(len(sub_map)) if sub_map[i] in sub_id]\r\n",
        "    # return the selected samples\r\n",
        "    return ix\r\n",
        "\r\n",
        "sub_map_subjects = np.unique(sub_map)\r\n",
        "sub_map_subjects_val = [[2,3,4,5],[3,4,5,6],[4,5,6,7],[5,6,7,8], [6,7,8,9],[7,8,9,10],[8,9,10,11],\r\n",
        "                        [9,10,11,12],[10,11,12,1],[11,12,1,2], [12,1,2,3], [1,2,3,4]]\r\n",
        "sub_map_subjects_test = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]]\r\n",
        "sub_map_subjects_train = []\r\n",
        "fold = len(sub_map_subjects_test)\r\n",
        "\r\n",
        "#Prepare data for 10-fold\r\n",
        "for i in range (fold):\r\n",
        "  train_temp =[]\r\n",
        "  for j in range (1,13):\r\n",
        "    if j not in sub_map_subjects_val[i] and j not in sub_map_subjects_test[i]:\r\n",
        "      train_temp.append(j)\r\n",
        "  sub_map_subjects_train.append(train_temp)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqEEOCkOGw6A"
      },
      "source": [
        "## The final section is for 10-fold experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXk1tpLMGvXn"
      },
      "source": [
        "#10-fold loop  \r\n",
        "for idx in range (0,fold):  \r\n",
        "  train_idx = data_for_subjects(sub_map, sub_map_subjects_train[idx])\r\n",
        "  val_idx = data_for_subjects(sub_map, sub_map_subjects_val[idx])\r\n",
        "  test_idx = data_for_subjects(sub_map, sub_map_subjects_test[idx])\r\n",
        "  session_label_test = session_label[test_idx]\r\n",
        "  # # Choose train val test set\r\n",
        "  x_amp_train = X_amp[train_idx]\r\n",
        "  x_amp_val = X_amp[val_idx]\r\n",
        "  x_amp_test = X_amp[test_idx]\r\n",
        "  y_train = y[train_idx]\r\n",
        "  y_val = y[val_idx]\r\n",
        "  y_test =  y[test_idx]\r\n",
        "\r\n",
        "  #Apply Data Augmentation\r\n",
        "  x_amp_train, y_train = increase_by_shift_time(x_amp_train, y_train)\r\n",
        "  x_amp_train, y_train = increase_by_shift_range(x_amp_train, y_train)\r\n",
        "  x_amp_train, y_train = increase_by_timewarp(x_amp_train, y_train)\r\n",
        "  x_amp_train, y_train = increase_by_magwarp(x_amp_train, y_train)\r\n",
        "\r\n",
        "  #Applying TD\r\n",
        "  x_diff_train = np.diff(x_amp_train, n=1,axis=2)\r\n",
        "  x_diff_val = np.diff(x_amp_val, n=1, axis=2)\r\n",
        "  x_diff_test = np.diff(x_amp_test, n=1, axis=2) \r\n",
        "  x_diff_train = x_diff_train.reshape(x_diff_train.shape[0], space*(time-1))\r\n",
        "  x_diff_val = x_diff_val.reshape(x_diff_val.shape[0], space*(time-1))\r\n",
        "  x_diff_test = x_diff_test.reshape(x_diff_test.shape[0], space*(time-1))\r\n",
        "\r\n",
        "\r\n",
        "  #Appying WRTFT\r\n",
        "  x_wrtft_train = apply_wrtft(x_amp_train)\r\n",
        "  x_wrtft_val = apply_wrtft(x_amp_val)\r\n",
        "  x_wrtft_test = apply_wrtft(x_amp_test)      \r\n",
        "  range_bins, time_bins = x_wrtft_train.shape[1], x_wrtft_train.shape[2]\r\n",
        "  x_wrtft_train = x_wrtft_train.reshape(-1, range_bins*time_bins)\r\n",
        "  x_wrtft_val = x_wrtft_val.reshape(-1, range_bins*time_bins)\r\n",
        "  x_wrtft_test = x_wrtft_test.reshape(-1, range_bins*time_bins)\r\n",
        "  scaler_wrtft = StandardScaler()\r\n",
        "  scaler_wrtft.fit(x_wrtft_train)\r\n",
        "  x_wrtft_train = scaler_wrtft.transform(x_wrtft_train)\r\n",
        "  x_wrtft_val = scaler_wrtft.transform(x_wrtft_val)\r\n",
        "  x_wrtft_test = scaler_wrtft.transform(x_wrtft_test)\r\n",
        "  x_wrtft_train = x_wrtft_train.reshape(-1, range_bins, time_bins, 1)\r\n",
        "  x_wrtft_val = x_wrtft_val.reshape(-1, range_bins, time_bins, 1)\r\n",
        "  x_wrtft_test = x_wrtft_test.reshape(-1, range_bins, time_bins, 1)\r\n",
        "\r\n",
        "\r\n",
        "  scaler_diff = StandardScaler()\r\n",
        "  scaler_diff.fit(x_diff_train)\r\n",
        "  def preprocess(X):\r\n",
        "    X = scaler_diff.transform(X)\r\n",
        "    X = X.reshape(X.shape[0], space, time-1, 1).astype('float32')\r\n",
        "    return X\r\n",
        "\r\n",
        "  x_diff_train = preprocess(x_diff_train)\r\n",
        "  x_diff_val = preprocess(x_diff_val)\r\n",
        "  x_diff_test = preprocess(x_diff_test)\r\n",
        "\r\n",
        "  \r\n",
        "  #Loss function and optimizer for DCNN models\r\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "  optimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)        \r\n",
        "\r\n",
        "  #Multiview & Task DCNN\r\n",
        "  modelNameMultiview = prefix + model_path\r\n",
        "  @tf.function\r\n",
        "  def Multiview_DCNN_train_step(x, y, model):\r\n",
        "      with tf.GradientTape() as tape:\r\n",
        "          predictions= model(x, training=True)\r\n",
        "          posture_loss = loss_object(y, predictions)\r\n",
        "          \r\n",
        "          loss = posture_loss\r\n",
        "          gradients = tape.gradient(loss, model.trainable_variables)\r\n",
        "      \r\n",
        "      optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
        "\r\n",
        "  def Multiview_DCNN_train(epochs, batch_size):\r\n",
        "      batch_count = int(x_diff_train.shape[0] / batch_size)\r\n",
        "      model = make_model()\r\n",
        "\r\n",
        "      best_epoch = 0\r\n",
        "      best_loss_val = 100000\r\n",
        "      count = 0\r\n",
        "      for e in range(1, epochs+1):\r\n",
        "          # print ('-'*15, 'Epoch %d' % e, '-'*15)\r\n",
        "          if count>PATIENCE:\r\n",
        "              break\r\n",
        "          for i in range(batch_count):\r\n",
        "\r\n",
        "              rand_nums = np.random.randint(0, x_diff_train.shape[0], size=batch_size)\r\n",
        "              x_diff_batch = tf.convert_to_tensor(x_diff_train[rand_nums])\r\n",
        "              x_wrtft_batch = tf.convert_to_tensor(x_wrtft_train[rand_nums])\r\n",
        "              y_batch = tf.convert_to_tensor(y_train[rand_nums])\r\n",
        "              \r\n",
        "\r\n",
        "              Multiview_DCNN_train_step([x_diff_batch, x_wrtft_batch], y_batch, model)\r\n",
        "              \r\n",
        "\r\n",
        "          #Compute loss for validation set\r\n",
        "          predictions_val = model([x_diff_val, x_wrtft_val], training=False)\r\n",
        "          loss_val = loss_object( tf.convert_to_tensor(y_val), predictions_val)\r\n",
        "          if loss_val<best_loss_val:\r\n",
        "              count=0\r\n",
        "              best_epoch = e\r\n",
        "              best_loss_val = loss_val\r\n",
        "              # print(best_loss_val)\r\n",
        "              model.save(modelNameMultiview)\r\n",
        "              # print('Model saved')\r\n",
        "          else:\r\n",
        "              count+=1\r\n",
        "\r\n",
        "  Multiview_DCNN_train(EPOCHS, BATCH_SIZE)\r\n",
        "  saved_multiview = load_model(modelNameMultiview, compile=False)\r\n",
        "  predictions_train = saved_multiview.predict([x_diff_train, x_wrtft_train], batch_size=128)\r\n",
        "  predictions_train = np.argmax(predictions_train, axis=1)\r\n",
        "\r\n",
        "  predictions_val = saved_multiview.predict([x_diff_val, x_wrtft_val])\r\n",
        "  predictions_val = np.argmax(predictions_val, axis=1)\r\n",
        "\r\n",
        "  predictions_test= saved_multiview.predict([x_diff_test, x_wrtft_test])\r\n",
        "  predictions_test = np.argmax(predictions_test, axis=1)\r\n",
        "\r\n",
        "  print('Train Acc=', accuracy_score(y_train, predictions_train))\r\n",
        "  print('Val Acc=', accuracy_score(y_val, predictions_val))\r\n",
        "  print('Session1 Test Acc=', accuracy_score(y_test[0:34], predictions_test[0:34]))\r\n",
        "  print('Session2 Test Acc=', accuracy_score(y_test[34:], predictions_test[34:]))\r\n",
        "  cm = confusion_matrix(y_test, predictions_test)\r\n",
        "  print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}